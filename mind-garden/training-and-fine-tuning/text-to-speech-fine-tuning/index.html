<!DOCTYPE html><html lang="en-US" class="astro-37fxchfa" style=""> <head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0, shrink-to-fit=no" name="viewport"><meta content="IE=edge" http-equiv="X-UA-Compatible"><title>Text-to-Speech (TTS) Fine-tuning • yose.is-a.dev</title><link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png"><link rel="preload" href="/_astro/avatar.Dix4fgQg.png" as="image" type="image/webp"><link rel="manifest" href="/favicon/site.webmanifest"><link rel="preload" href="/fonts/Satoshi/Satoshi-Variable.ttf" as="font" type="font/ttf" crossorigin><link rel="preload" href="/fonts/Satoshi/Satoshi-VariableItalic.ttf" as="font" type="font/ttf" crossorigin><link rel="canonical" href="https://astro-pure.js.org/mind-garden/training-and-fine-tuning/text-to-speech-fine-tuning"><meta content="Text-to-Speech (TTS) Fine-tuning • yose.is-a.dev" name="title"><meta content="Learn how to fine-tune Text-to-Speech (TTS) models to adapt them to your specific dataset, use case, or desired style and tone." name="description"><meta content="Yose Marthin Giyay" name="author"><meta content="" name="theme-color"><meta content="website" property="og:type"><meta content="Text-to-Speech (TTS) Fine-tuning" property="og:title"><meta content="Learn how to fine-tune Text-to-Speech (TTS) models to adapt them to your specific dataset, use case, or desired style and tone." property="og:description"><meta content="https://astro-pure.js.org/mind-garden/training-and-fine-tuning/text-to-speech-fine-tuning" property="og:url"><meta content="yose.is-a.dev" property="og:site_name"><meta content="en_US" property="og:locale"><meta content="https://astro-pure.js.org/images/social-card.png" property="og:image"><meta content="1200" property="og:image:width"><meta content="630" property="og:image:height"><meta content="summary_large_image" property="twitter:card"><meta content="https://astro-pure.js.org/mind-garden/training-and-fine-tuning/text-to-speech-fine-tuning" property="twitter:url"><meta content="Text-to-Speech (TTS) Fine-tuning" property="twitter:title"><meta content="Learn how to fine-tune Text-to-Speech (TTS) models to adapt them to your specific dataset, use case, or desired style and tone." property="twitter:description"><meta content="https://astro-pure.js.org/images/social-card.png" property="twitter:image"><link href="/sitemap-index.xml" rel="sitemap"><link rel="alternate" type="application/rss+xml" title="yose.is-a.dev" href="https://astro-pure.js.org/rss.xml"><meta content="Astro v5.14.5" name="generator"><link rel="stylesheet" href="/styles/global.css"><script type="module">console.log("%c Astro Theme Pure %c https://github.com/cworld1/astro-theme-pure/","color:#fff;background:linear-gradient(90deg,#448bff,#44e9ff);padding:5px 0;","color:#000;background:linear-gradient(90deg,#44e9ff,#ffffff);padding:5px 10px 5px 0px;");</script><script>
  function simpleSetTheme() {
    let theme = localStorage.getItem('theme')
    // If undefined or 'system', get from system
    if (!theme || theme === 'system') {
      theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light'
    }
    document.documentElement.classList.toggle('dark', theme === 'dark')
    document
      .querySelector('meta[name="theme-color"]')
      ?.setAttribute('content', theme === 'dark' ? '#0B0B10' : '#FCFCFD')
  }
  simpleSetTheme()
  document.addEventListener('astro:page-load', () => simpleSetTheme())
</script><script type="module" src="/_astro/ThemeProvider.astro_astro_type_script_index_0_lang.CW1wQWPM.js"></script><link rel="stylesheet" href="/_astro/index.oNOocK2m.css">
<style>@media (min-width: 1300px){main,.bottom{transform:translate(7.9rem)}#sidebar{overflow:inherit}toc-heading{height:100%;display:flex;flex-direction:column}toc-heading>ul{overflow-y:scroll}.docs-toc:where(.astro-u7fw3ohz){position:fixed;top:0;left:-64rem;margin-top:0;max-height:calc(100vh - 7rem);overflow-y:scroll;padding-right:1rem!important}}
</style>
<link rel="stylesheet" href="/_astro/_id_.v6sYZ8Ej.css">
<style>docs-toc:where(.astro-xsy4z74c) .docs-item:where(.astro-xsy4z74c):before{content:"";display:block;position:absolute;top:5%;bottom:5%;left:-.5rem;width:2px;background-color:hsl(var(--input) / var(--un-bg-opacity))}docs-toc:where(.astro-xsy4z74c) .docs-item.docs-hl{background-color:hsl(var(--muted) / var(--un-bg-opacity));font-weight:500;& a:where(.astro-xsy4z74c){color:hsl(var(--primary) / var(--un-text-opacity))}&:before{background-color:hsl(var(--primary) / var(--un-bg-opacity))}}
@media (min-width: 769px){#sidebar-shade:where(.astro-scuu7fyy){display:none!important}}@media (max-width: 769px){aside:where(.astro-scuu7fyy).show{display:block!important;opacity:1}@keyframes fade-in-side{0%{transform:translate(50%);opacity:0}to{transform:translate(0);opacity:1}}@keyframes fade-in{0%{opacity:0}to{opacity:1}}aside:where(.astro-scuu7fyy){animation:.3s fade-in-side!important;animation-fill-mode:forwards;right:0;top:0!important;height:100vh!important;min-width:40vw!important;max-width:75vw!important;border-top-right-radius:0!important;border-bottom-right-radius:0!important;padding-top:1.5rem!important;padding-bottom:1.5rem!important}#sidebar-shade:where(.astro-scuu7fyy){animation:.3s fade-in}}@media (max-width: 640px){aside:where(.astro-scuu7fyy){min-width:55vw!important}}
header-component:where(.astro-zgz2aqyx){transition:padding .3s,transform .3s,margin-inline .3s,border .15s,background-color .15s;&.not-top{--un-border-opacity: 1;border-color:hsl(var(--border) / var(--un-border-opacity));--un-bg-opacity: 1;background-color:hsl(var(--background) / var(--un-bg-opacity));padding-left:.375rem;padding-right:.375rem;box-shadow:#fff 0 0,#18181b14 0 0 0 1px,#27272a14 0 10px 15px -3px,#27272a14 0 4px 6px -4px}&[data-show=false]:not(.expanded){transform:translateY(-5rem)}}@media (min-width: 800px){header-component:where(.astro-zgz2aqyx).not-top{margin-inline:8%}}.dark header-component:where(.astro-zgz2aqyx).not-top{background-color:hsl(var(--muted) / var(--un-bg-opacity))}@media (max-width: 640px){#headerExpandContent:where(.astro-zgz2aqyx){grid-template-rows:0fr;transition:opacity .3s,padding .3s,border-color .15s,grid-template-rows .3s}.expanded:where(.astro-zgz2aqyx) #headerExpandContent:where(.astro-zgz2aqyx){grid-template-rows:1fr}.expanded:where(.astro-zgz2aqyx).not-top #headerExpandContent:where(.astro-zgz2aqyx){box-shadow:#fff 0 0,#18181b14 0 0 0 1px,#27272a14 0 10px 15px -3px,#27272a14 0 4px 6px -4px}header-component:where(.astro-zgz2aqyx) #headerExpandContent:where(.astro-zgz2aqyx):after{box-sizing:content-box;content:"";position:absolute;inset-inline:calc(-1rem - 1px);bottom:0;top:-5rem;z-index:-1;transition:.3s;visibility:hidden;opacity:0;border-bottom:1px solid transparent}header-component:where(.astro-zgz2aqyx):not(.not-top) #headerExpandContent:where(.astro-zgz2aqyx):after{visibility:visible;bottom:-1rem;opacity:1;background-color:hsl(var(--muted) / var(--un-bg-opacity, 1));border-bottom-color:hsl(var(--border) / var(--un-border-opacity, 1))}}#toggleDarkMode:where(.astro-zgz2aqyx){&[data-theme=dark]{.system:where(.astro-zgz2aqyx){display:none}.dark:where(.astro-zgz2aqyx){display:block}}&[data-theme=light]{.system:where(.astro-zgz2aqyx){display:none}.light{display:block}}}@font-face{font-family:Satoshi;src:url(/fonts/Satoshi/Satoshi-Variable.ttf) format("truetype");font-style:normal;font-display:swap}@font-face{font-family:Satoshi;src:url(/fonts/Satoshi/Satoshi-VariableItalic.ttf) format("truetype");font-style:italic;font-display:swap}html{font-family:Satoshi,serif}:root{--background: 210 33% 99%;--foreground: 240 10% 3.9%;--card: 0 0% 100%;--card-foreground: 240 10% 3.9%;--popover: 0 0% 100%;--popover-foreground: 240 10% 3.9%;--primary: 200 29% 45%;--primary-foreground: 0 0% 92.5%;--secondary: 240 4.8% 95.9%;--secondary-foreground: 240 5.9% 10%;--muted: 240 4.8% 95%;--muted-foreground: 240 3.8% 28.1%;--accent: 240 4.8% 95.9%;--accent-foreground: 240 5.9% 10%;--destructive: 0 72.22% 50.59%;--destructive-foreground: 0 0% 98%;--border: 240 5.9% 88%;--input: 240 5.9% 90%;--ring: 240 5.9% 10%;--radius: .5rem}.dark{--background: 240 20.54% 5.2%;--foreground: 0 0% 90%;--card: 240 10% 3.9%;--card-foreground: 0 0% 98%;--popover: 240 10% 3.9%;--popover-foreground: 0 0% 98%;--primary: 195 95% 85%;--primary-foreground: 240 3.7% 15.9%;--secondary: 240 3.7% 15.9%;--secondary-foreground: 0 0% 98%;--muted: 240 5.9% 12%;--muted-foreground: 240 5% 74.9%;--accent: 240 3.7% 15.9%;--accent-foreground: 0 0% 98%;--destructive: 0 62.8% 30.6%;--destructive-foreground: 0 0% 98%;--border: 240 3.7% 19.9%;--input: 240 3.7% 15.9%;--ring: 240 4.9% 83.9%}:root{--un-default-border-color: hsl(var(--border) / 1)}html.dark{color-scheme:dark}a{transition:color .2s ease;&:hover{color:hsl(var(--primary) / var(--un-text-opacity, 1))}}.prose p,.prose li,.prose blockquote{font-size:1.2rem;line-height:1.7}.highlight{color:var(--highlightColor, hsl(var(--primary) / var(--un-text-opacity)))!important}.highlight-bg{background-color:var( --highlightColor, hsl(var(--primary) / var(--un-text-opacity)) )!important}
@keyframes pulsate{0%{opacity:1}50%{opacity:.4}to{opacity:1}}.loading:where(.astro-fkh43kdv) .load-block:where(.astro-fkh43kdv){color:transparent;border-radius:calc(var(--radius) - 3px);background-color:hsl(var(--primary-foreground) / var(--un-bg-opacity, 1));animation:pulsate 2s infinite linear;-webkit-user-select:none;-moz-user-select:none;user-select:none}.loading:where(.astro-fkh43kdv) .load-block:where(.astro-fkh43kdv):nth-child(2){animation-delay:1s}.no-license:where(.astro-fkh43kdv){opacity:.5}:not(.loading):where(.astro-fkh43kdv) #gh-avatar:where(.astro-fkh43kdv){background-color:hsl(var(--primary-foreground) / var(--un-bg-opacity))}
:where([data-astro-image]){-o-object-fit:var(--fit);object-fit:var(--fit);-o-object-position:var(--pos);object-position:var(--pos);height:auto}:where([data-astro-image=full-width]){width:100%}:where([data-astro-image=constrained]){max-width:100%}
.link-preview:where(.astro-fov3225h) img:where(.astro-fov3225h),.link-preview:where(.astro-fov3225h) video:where(.astro-fov3225h){aspect-ratio:1200 / 630;width:100%;height:auto;-o-object-fit:cover;object-fit:cover}
.medium-zoom-overlay{position:fixed;inset:0;opacity:0;transition:opacity .3s;will-change:opacity}.medium-zoom--opened .medium-zoom-overlay{cursor:pointer;cursor:zoom-out;opacity:1;z-index:999}.medium-zoom-image{cursor:pointer;cursor:zoom-in;transition:transform .3s cubic-bezier(.2,0,.2,1)!important}.medium-zoom-image--hidden{visibility:hidden}.medium-zoom-image--opened{position:relative;cursor:pointer;cursor:zoom-out;will-change:transform;z-index:999}
starlight-tabs:where(.astro-m7f3nve7){display:block}.tablist-wrapper:where(.astro-m7f3nve7){overflow-x:auto}:where(.astro-m7f3nve7)[role=tablist]{display:flex;list-style:none;border-bottom:2px solid hsl(var(--border) / var(--un-border-opacity, 1));padding:0}.tab:where(.astro-m7f3nve7){margin-bottom:-2px}.tab:where(.astro-m7f3nve7)>:where(.astro-m7f3nve7)[role=tab]{display:flex;align-items:center;gap:.5rem;padding:.2rem 1.25rem;text-decoration:none;border-bottom:2px solid hsl(var(--border) / var(--un-border-opacity, 1));color:hsl(var(--foreground) / var(--un-text-opacity, 1));outline-offset:-.1875rem;overflow-wrap:initial}.tab:where(.astro-m7f3nve7) :where(.astro-m7f3nve7)[role=tab][aria-selected=true]{color:hsl(var(--primary) / var(--un-text-opacity, 1));border-color:hsl(var(--primary) / var(--un-text-opacity, 1));font-weight:600}.tablist-wrapper:where(.astro-m7f3nve7)~[role=tabpanel]{margin-top:1rem}
.aside:where(.astro-kdde4gve)>.aside-container:where(.astro-kdde4gve){--un-bg-opacity: .07;&.aside-tip{--primary: 234 60% 60%}&.aside-caution{--primary: 41 90% 50%}&.aside-danger{--primary: 339 90% 60%}.aside-content{>:first-child{margin-top:0}>:last-child{margin-bottom:0}}}
.mdx-repl:where(.astro-le7uvupp){background:linear-gradient(135deg,hsl(var(--primary) / .05),hsl(var(--muted) / .2))}.mdx-repl-container:where(.astro-le7uvupp)>*{width:var(--width)}.mdx-repl-container:where(.astro-le7uvupp)>*:first-child{margin-top:0}.mdx-repl-container:where(.astro-le7uvupp)>*:last-child{margin-bottom:0}.mdx-repl:where(.astro-le7uvupp) .astro-code{margin:0;border-radius:0}.mdx-repl:where(.astro-le7uvupp) div[role=tabpanel]{margin-top:0}
svg:where(.astro-hn7k2gay){color:var(--sl-icon-color);font-size:var(--sl-icon-size, 1em);width:1.5em;height:1.5em}
.expand-content:where(.astro-5lfx4xea){grid-template-rows:0fr}.expanded:where(.astro-5lfx4xea) .expand-content:where(.astro-5lfx4xea){grid-template-rows:1fr}
.sl-steps{--bullet-size: 1.75rem ;--bullet-margin: .375rem;list-style:none!important;counter-reset:steps-counter var(--sl-steps-start, 0);padding-inline-start:0!important}.sl-steps>li{counter-increment:steps-counter;position:relative;padding-inline-start:calc(var(--bullet-size) + 1rem);padding-bottom:1px;min-height:calc(var(--bullet-size) + var(--bullet-margin))}.sl-steps>li+li{margin-top:0}.sl-steps>li:before{content:counter(steps-counter);position:absolute;top:0;inset-inline-start:0;width:var(--bullet-size);height:var(--bullet-size);line-height:var(--bullet-size);font-size:.8125rem;font-weight:600;text-align:center;color:hsl(var(--foreground) / var(--un-text-opacity, 1));background-color:hsl(var(--primary-foreground) / var(--un-bg-opacity, 1));border-radius:99rem;box-shadow:inset 0 0 0 1px hsl(var(--border) / var(--un-border-opacity, 1))}.sl-steps>li:after{--guide-width: 1px;content:"";position:absolute;top:calc(var(--bullet-size) + var(--bullet-margin));bottom:var(--bullet-margin);inset-inline-start:calc((var(--bullet-size) - var(--guide-width)) / 2);width:var(--guide-width);background-color:hsl(var(--border) / var(--un-border-opacity, 1))}.sl-steps>li>:first-child{--lh: 1.75em ;--shift-y: calc(.5 * (var(--bullet-size) - var(--lh)));margin-top:0;transform:translateY(var(--shift-y));margin-bottom:var(--shift-y);color:hsl(var(--foreground) / var(--un-text-opacity, 1))}.sl-steps>li>:first-child:where(h1,h2,h3,h4,h5,h6){--lh: 1.2em }
</style><script type="module" src="/_astro/page.CwTOY3h6.js"></script></head> <body class="flex justify-center bg-background text-foreground astro-37fxchfa" class="astro-scuu7fyy" style="">  <div class="w-full max-w-[70rem] px-4 sm:px-7 lg:px-10 min-h-[100dvh] flex flex-col justify-between astro-37fxchfa" style=""> <header-component class="group sticky top-4 z-50 max-md:z-30 mb-12 flex items-center justify-between rounded-xl border border-transparent max-sm:py-1 sm:rounded-2xl astro-zgz2aqyx"> <a class="z-30 text-xl font-semibold group-[.not-top]:ms-2 sm:group-[.not-top]:ms-3 astro-zgz2aqyx" style="transition:margin-inline 0.3s" href="/" aria-label="Brand">yose.is-a.dev</a> <div class="flex items-center gap-x-2 astro-zgz2aqyx">  <div id="headerExpandContent" class="end-0 start-0 top-12 grid border border-transparent group-[.not-top]:rounded-xl group-[.expanded]:opacity-100 dark:group-[.expanded.not-top]:bg-muted max-sm:absolute max-sm:opacity-0 max-sm:group-[.not-top]:border-border max-sm:group-[.expanded.not-top]:bg-background max-sm:group-[.not-top]:px-4 max-sm:group-[.not-top]:py-2 sm:grid-rows-1 astro-zgz2aqyx"> <div class="flex flex-col items-center justify-center overflow-hidden sm:flex-row astro-zgz2aqyx"> <a href="/blog" class="w-full flex-none grow py-2 text-right font-medium transition-none hover:text-primary sm:w-fit sm:px-3 astro-zgz2aqyx" aria-label="Nav menu item"> Blog </a><a href="/mind-garden" class="w-full flex-none grow py-2 text-right font-medium transition-none hover:text-primary sm:w-fit sm:px-3 astro-zgz2aqyx" aria-label="Nav menu item"> Mind Garden </a><a href="/projects" class="w-full flex-none grow py-2 text-right font-medium transition-none hover:text-primary sm:w-fit sm:px-3 astro-zgz2aqyx" aria-label="Nav menu item"> Projects </a><a href="/media" class="w-full flex-none grow py-2 text-right font-medium transition-none hover:text-primary sm:w-fit sm:px-3 astro-zgz2aqyx" aria-label="Nav menu item"> Movies &amp; Books </a><a href="/about" class="w-full flex-none grow py-2 text-right font-medium transition-none hover:text-primary sm:w-fit sm:px-3 astro-zgz2aqyx" aria-label="Nav menu item"> About </a> <div class="flex w-full grow flex-row justify-end gap-x-3 sm:w-fit sm:gap-x-5 astro-zgz2aqyx"> <a class="px-1 py-2 transition-none sm:px-2 astro-zgz2aqyx" href="/search" title="Search"> <span class="sr-only astro-zgz2aqyx">Search</span> <svg aria-hidden="true" class="size-5 astro-zgz2aqyx astro-hn7k2gay" width="22" height="22" viewBox="0 0 24 24" fill="currentColor" style="--sl-icon-size: 1em;"><g fill="none" fill-rule="evenodd"><path d="m12.593 23.258l-.011.002l-.071.035l-.02.004l-.014-.004l-.071-.035q-.016-.005-.024.005l-.004.01l-.017.428l.005.02l.01.013l.104.074l.015.004l.012-.004l.104-.074l.012-.016l.004-.017l-.017-.427q-.004-.016-.017-.018m.265-.113l-.013.002l-.185.093l-.01.01l-.003.011l.018.43l.005.012l.008.007l.201.093q.019.005.029-.008l.004-.014l-.034-.614q-.005-.018-.02-.022m-.715.002a.02.02 0 0 0-.027.006l-.006.014l-.034.614q.001.018.017.024l.015-.002l.201-.093l.01-.008l.004-.011l.017-.43l-.003-.012l-.01-.01z"/><path fill="currentColor" d="M5 10a5 5 0 1 1 10 0a5 5 0 0 1-10 0m5-7a7 7 0 1 0 4.192 12.606l5.1 5.101a1 1 0 0 0 1.415-1.414l-5.1-5.1A7 7 0 0 0 10 3"/></g></svg>  </a> </div> </div> </div>  <div class="z-30 flex gap-x-4 group-[.not-top]:gap-x-2 astro-zgz2aqyx" style="transition:gap 0.3s"> <button id="toggleDarkMode" class="group/dark box-content size-5 rounded-md border p-1.5 transition-colors hover:bg-border sm:group-[.not-top]:rounded-xl astro-zgz2aqyx"> <span class="sr-only astro-zgz2aqyx">Dark Theme</span> <svg aria-hidden="true" class="system size-5 group-hover/dark:text-primary astro-zgz2aqyx astro-hn7k2gay" width="22" height="22" viewBox="0 0 24 24" fill="currentColor" style="--sl-icon-size: 1em;"><g fill="none" fill-rule="evenodd"><path d="m12.593 23.258l-.011.002l-.071.035l-.02.004l-.014-.004l-.071-.035q-.016-.005-.024.005l-.004.01l-.017.428l.005.02l.01.013l.104.074l.015.004l.012-.004l.104-.074l.012-.016l.004-.017l-.017-.427q-.004-.016-.017-.018m.265-.113l-.013.002l-.185.093l-.01.01l-.003.011l.018.43l.005.012l.008.007l.201.093q.019.005.029-.008l.004-.014l-.034-.614q-.005-.018-.02-.022m-.715.002a.02.02 0 0 0-.027.006l-.006.014l-.034.614q.001.018.017.024l.015-.002l.201-.093l.01-.008l.004-.011l.017-.43l-.003-.012l-.01-.01z"/><path fill="currentColor" fill-rule="nonzero" d="M19 3a2 2 0 0 1 2 2v11a2 2 0 0 1-2 2h-4v1h1a1 1 0 1 1 0 2H8a1 1 0 1 1 0-2h1v-1H5a2 2 0 0 1-2-2V5a2 2 0 0 1 2-2Zm-6 15h-2v1h2zm6-13H5v11h14z"/></g></svg>  <svg aria-hidden="true" class="light hidden size-5 group-hover/dark:text-primary astro-zgz2aqyx astro-hn7k2gay" width="22" height="22" viewBox="0 0 24 24" fill="currentColor" style="--sl-icon-size: 1em;"><g fill="none"><path d="m12.593 23.258l-.011.002l-.071.035l-.02.004l-.014-.004l-.071-.035q-.016-.005-.024.005l-.004.01l-.017.428l.005.02l.01.013l.104.074l.015.004l.012-.004l.104-.074l.012-.016l.004-.017l-.017-.427q-.004-.016-.017-.018m.265-.113l-.013.002l-.185.093l-.01.01l-.003.011l.018.43l.005.012l.008.007l.201.093q.019.005.029-.008l.004-.014l-.034-.614q-.005-.018-.02-.022m-.715.002a.02.02 0 0 0-.027.006l-.006.014l-.034.614q.001.018.017.024l.015-.002l.201-.093l.01-.008l.004-.011l.017-.43l-.003-.012l-.01-.01z"/><path fill="currentColor" d="M12 19a1 1 0 0 1 1 1v1a1 1 0 1 1-2 0v-1a1 1 0 0 1 1-1m6.364-2.05l.707.707a1 1 0 0 1-1.414 1.414l-.707-.707a1 1 0 0 1 1.414-1.414m-12.728 0a1 1 0 0 1 1.497 1.32l-.083.094l-.707.707a1 1 0 0 1-1.497-1.32l.083-.094zM12 6a6 6 0 1 1 0 12a6 6 0 0 1 0-12m0 2a4 4 0 1 0 0 8a4 4 0 0 0 0-8m-8 3a1 1 0 0 1 .117 1.993L4 13H3a1 1 0 0 1-.117-1.993L3 11zm17 0a1 1 0 1 1 0 2h-1a1 1 0 1 1 0-2zM4.929 4.929a1 1 0 0 1 1.32-.083l.094.083l.707.707a1 1 0 0 1-1.32 1.497l-.094-.083l-.707-.707a1 1 0 0 1 0-1.414m14.142 0a1 1 0 0 1 0 1.414l-.707.707a1 1 0 1 1-1.414-1.414l.707-.707a1 1 0 0 1 1.414 0M12 2a1 1 0 0 1 1 1v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1"/></g></svg>  <svg aria-hidden="true" class="dark hidden size-5 group-hover/dark:text-primary astro-zgz2aqyx astro-hn7k2gay" width="22" height="22" viewBox="0 0 24 24" fill="currentColor" style="--sl-icon-size: 1em;"><g fill="none" fill-rule="evenodd"><path d="m12.593 23.258l-.011.002l-.071.035l-.02.004l-.014-.004l-.071-.035q-.016-.005-.024.005l-.004.01l-.017.428l.005.02l.01.013l.104.074l.015.004l.012-.004l.104-.074l.012-.016l.004-.017l-.017-.427q-.004-.016-.017-.018m.265-.113l-.013.002l-.185.093l-.01.01l-.003.011l.018.43l.005.012l.008.007l.201.093q.019.005.029-.008l.004-.014l-.034-.614q-.005-.018-.02-.022m-.715.002a.02.02 0 0 0-.027.006l-.006.014l-.034.614q.001.018.017.024l.015-.002l.201-.093l.01-.008l.004-.011l.017-.43l-.003-.012l-.01-.01z"/><path fill="currentColor" d="M12.477 4.546a1.01 1.01 0 0 1 1.097-1.409A9 9 0 0 1 12 21c-4.434 0-8.118-3.206-8.863-7.426a1.01 1.01 0 0 1 1.409-1.097a6 6 0 0 0 7.931-7.931m2.404 1.072a8 8 0 0 1-9.263 9.263A7.002 7.002 0 0 0 19 12.001a7 7 0 0 0-4.12-6.383ZM5.565 7.716l.064.14a3.26 3.26 0 0 0 1.237 1.363l.1.059a.068.068 0 0 1 0 .118l-.1.058a3.26 3.26 0 0 0-1.237 1.364l-.064.14a.071.071 0 0 1-.13 0l-.064-.14a3.26 3.26 0 0 0-1.237-1.364l-.1-.058a.068.068 0 0 1 0-.118l.1-.059A3.26 3.26 0 0 0 5.37 7.855l.064-.14a.071.071 0 0 1 .13 0Zm2.832-4.859c.04-.09.166-.09.206 0l.102.222a5.2 5.2 0 0 0 1.97 2.172l.157.092a.108.108 0 0 1 0 .189l-.158.092a5.2 5.2 0 0 0-2.07 2.394a.113.113 0 0 1-.207 0l-.102-.222a5.2 5.2 0 0 0-1.97-2.172l-.158-.092a.108.108 0 0 1 0-.189l.158-.092a5.2 5.2 0 0 0 1.97-2.172z"/></g></svg>  </button> <button id="toggleMenu" class="rounded-md border p-1.5 transition-colors hover:bg-border sm:hidden sm:group-[.not-top]:rounded-xl astro-zgz2aqyx"> <span class="sr-only astro-zgz2aqyx">Menu</span> <svg aria-hidden="true" class="size-5 astro-zgz2aqyx astro-hn7k2gay" width="22" height="22" viewBox="0 0 24 24" fill="currentColor" style="--sl-icon-size: 1em;"><g fill="none"><path d="m12.593 23.258l-.011.002l-.071.035l-.02.004l-.014-.004l-.071-.035q-.016-.005-.024.005l-.004.01l-.017.428l.005.02l.01.013l.104.074l.015.004l.012-.004l.104-.074l.012-.016l.004-.017l-.017-.427q-.004-.016-.017-.018m.265-.113l-.013.002l-.185.093l-.01.01l-.003.011l.018.43l.005.012l.008.007l.201.093q.019.005.029-.008l.004-.014l-.034-.614q-.005-.018-.02-.022m-.715.002a.02.02 0 0 0-.027.006l-.006.014l-.034.614q.001.018.017.024l.015-.002l.201-.093l.01-.008l.004-.011l.017-.43l-.003-.012l-.01-.01z"/><path fill="currentColor" d="M20 18a1 1 0 0 1 .117 1.993L20 20H4a1 1 0 0 1-.117-1.993L4 18zm0-7a1 1 0 1 1 0 2H4a1 1 0 1 1 0-2zm0-7a1 1 0 1 1 0 2H4a1 1 0 0 1 0-2z"/></g></svg>  </button> </div> </div> </header-component>  <script>
  const toggleDarkModeElement = document.getElementById('toggleDarkMode')
  if (toggleDarkModeElement) {
    toggleDarkModeElement.dataset.theme = localStorage.getItem('theme') || 'system'
  }
</script> <script type="module" src="/_astro/Header.astro_astro_type_script_index_0_lang.52pHqpSo.js"></script>  <div class="flex-1 w-full astro-37fxchfa" style="">  <a class="group inline-flex items-center gap-x-1 rounded-lg bg-muted border px-2 py-1 text-sm text-muted-foreground transition-all hover:bg-primary-foreground no-underline astro-scuu7fyy" href="/mind-garden" data-astro-prefetch="true">  <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round" class="stroke-muted-foreground group-hover:stroke-primary"> <line x1="19" y1="12" x2="5" y2="12" class="translate-x-3 scale-x-0 transition-all duration-300 ease-in-out group-hover:translate-x-0 group-hover:scale-x-100"></line> <polyline points="12 19 5 12 12 5" class="translate-x-1 transition-all duration-300 ease-in-out group-hover:translate-x-0"></polyline> </svg>   <p class="my-0">Back</p>     </a> <main class="mt-6 items-start gap-x-10 md:flex astro-scuu7fyy">  <aside class="animate top-20 min-w-48 basis-60 overflow-y-scroll max-md:hidden z-40 fixed md:sticky md:order-2 lg:shrink-0 max-md:border max-md:px-4 max-md:py-3 max-md:bg-muted max-md:rounded-xl astro-scuu7fyy" style="height:calc(100vh - 7rem);" id="sidebar"> <toc-heading class="astro-u7fw3ohz astro-mmcgqrmu"> <h2 class="font-semibold astro-mmcgqrmu">TABLE OF CONTENTS</h2> <ul class="mt-4 text-card-foreground astro-mmcgqrmu"> <li> <div class="relative"> <span class="absolute top-[5%] w-[2px] rounded transition-colors duration-300 [&.highlight-bg]:bg-primary [&.is-read]:bg-input"></span> <a aria-label="Scroll to section: Fine-tuning Notebooks:" class="toc-item line-clamp-2 px-3 py-1 ms-2 text-foreground/75 transition-all hover:text-foreground [&#38;.highlight]:font-medium [&#38;.highlight]:text-primary [&#38;.is-read]:text-input [&#38;.highlight-bg-translucent]:bg-muted" href="#fine-tuning-notebooks">Fine-tuning Notebooks:</a> </div>  </li><li> <div class="relative"> <span class="absolute top-[5%] w-[2px] rounded transition-colors duration-300 [&.highlight-bg]:bg-primary [&.is-read]:bg-input"></span> <a aria-label="Scroll to section: Choosing and Loading a TTS Model" class="toc-item line-clamp-2 px-3 py-1 ms-2 text-foreground/75 transition-all hover:text-foreground [&#38;.highlight]:font-medium [&#38;.highlight]:text-primary [&#38;.is-read]:text-input [&#38;.highlight-bg-translucent]:bg-muted" href="#choosing-and-loading-a-tts-model">Choosing and Loading a TTS Model</a> </div> <ul> <li> <div class="relative"> <span class="absolute top-[5%] w-[2px] rounded transition-colors duration-300 [&.highlight-bg]:bg-primary [&.is-read]:bg-input"></span> <a aria-label="Scroll to section: Sesame-CSM (1B) Details" class="toc-item line-clamp-2 px-3 py-1 ms-2 text-foreground/75 transition-all hover:text-foreground [&#38;.highlight]:font-medium [&#38;.highlight]:text-primary [&#38;.is-read]:text-input [&#38;.highlight-bg-translucent]:bg-muted ps-7" href="#sesame-csm-1b-details">Sesame-CSM (1B) Details</a> </div>  </li><li> <div class="relative"> <span class="absolute top-[5%] w-[2px] rounded transition-colors duration-300 [&.highlight-bg]:bg-primary [&.is-read]:bg-input"></span> <a aria-label="Scroll to section: Orpheus-TTS (3B) Details" class="toc-item line-clamp-2 px-3 py-1 ms-2 text-foreground/75 transition-all hover:text-foreground [&#38;.highlight]:font-medium [&#38;.highlight]:text-primary [&#38;.is-read]:text-input [&#38;.highlight-bg-translucent]:bg-muted ps-7" href="#orpheus-tts-3b-details">Orpheus-TTS (3B) Details</a> </div>  </li><li> <div class="relative"> <span class="absolute top-[5%] w-[2px] rounded transition-colors duration-300 [&.highlight-bg]:bg-primary [&.is-read]:bg-input"></span> <a aria-label="Scroll to section: Loading the models" class="toc-item line-clamp-2 px-3 py-1 ms-2 text-foreground/75 transition-all hover:text-foreground [&#38;.highlight]:font-medium [&#38;.highlight]:text-primary [&#38;.is-read]:text-input [&#38;.highlight-bg-translucent]:bg-muted ps-7" href="#loading-the-models">Loading the models</a> </div>  </li> </ul> </li><li> <div class="relative"> <span class="absolute top-[5%] w-[2px] rounded transition-colors duration-300 [&.highlight-bg]:bg-primary [&.is-read]:bg-input"></span> <a aria-label="Scroll to section: Preparing Your Dataset" class="toc-item line-clamp-2 px-3 py-1 ms-2 text-foreground/75 transition-all hover:text-foreground [&#38;.highlight]:font-medium [&#38;.highlight]:text-primary [&#38;.is-read]:text-input [&#38;.highlight-bg-translucent]:bg-muted" href="#preparing-your-dataset">Preparing Your Dataset</a> </div>  </li><li> <div class="relative"> <span class="absolute top-[5%] w-[2px] rounded transition-colors duration-300 [&.highlight-bg]:bg-primary [&.is-read]:bg-input"></span> <a aria-label="Scroll to section: Fine-Tuning TTS with Unsloth" class="toc-item line-clamp-2 px-3 py-1 ms-2 text-foreground/75 transition-all hover:text-foreground [&#38;.highlight]:font-medium [&#38;.highlight]:text-primary [&#38;.is-read]:text-input [&#38;.highlight-bg-translucent]:bg-muted" href="#fine-tuning-tts-with-unsloth">Fine-Tuning TTS with Unsloth</a> </div>  </li><li> <div class="relative"> <span class="absolute top-[5%] w-[2px] rounded transition-colors duration-300 [&.highlight-bg]:bg-primary [&.is-read]:bg-input"></span> <a aria-label="Scroll to section: Fine-tuning Voice models vs. Zero-shot voice cloning" class="toc-item line-clamp-2 px-3 py-1 ms-2 text-foreground/75 transition-all hover:text-foreground [&#38;.highlight]:font-medium [&#38;.highlight]:text-primary [&#38;.is-read]:text-input [&#38;.highlight-bg-translucent]:bg-muted" href="#fine-tuning-voice-models-vs-zero-shot-voice-cloning">Fine-tuning Voice models vs. Zero-shot voice cloning</a> </div>  </li> </ul> </toc-heading> <script type="module">class l extends HTMLElement{constructor(){super(),this.headings=[],this.tocLinks=[],this.headingProgress={},this.updatePositionAndStyle=()=>{const t=window.innerHeight,r=window.scrollY-(document.querySelector("#content")?.offsetTop||0),o=(document.querySelector("#content")?.offsetHeight||0)+127;this.headings.forEach((e,n)=>{const h=this.headings[n+1]?.offsetTop||o,s=[e.offsetTop-r,h-r-e.offsetHeight],i=(t-s[0])/(s[1]-s[0]);this.headingProgress[e.id]={inView:s[0]<t&&s[1]>0,progress:Math.max(0,Math.min(1,i))}}),this.tocLinks.forEach(({element:e,progressBar:n,slug:h},s)=>{const{inView:i,progress:g}=this.headingProgress[h];this.headingProgress[h]&&(e.classList.toggle("highlight",i),e.classList.toggle("highlight-bg-translucent",i),e.classList.toggle("rounded-t-2xl",i&&(s==0||!this.headingProgress[this.tocLinks[s-1]?.slug].inView)),e.classList.toggle("rounded-b-2xl",i&&(s==this.tocLinks.length-1||!this.headingProgress[this.tocLinks[s+1]?.slug].inView)),n.classList.toggle("is-read",!i&&g==1),n.classList.toggle("highlight-bg",i),n.style.setProperty("height",`${g*90}%`))})},this.headings=Array.from(document.querySelectorAll("article h2, article h3, article h4, article h5, article h6")),this.tocLinks=Array.from(this.querySelectorAll('a[href^="#"]')).map(t=>({element:t,progressBar:t.previousElementSibling,slug:(t.getAttribute("href")||"").substring(1)}))}connectedCallback(){this.tocLinks.forEach(t=>{t.element.addEventListener("click",r=>{r.preventDefault();const o=this.headings.find(e=>e.id===t.slug);o?(history.pushState(null,o.textContent||"",t.element.getAttribute("href")),o.scrollIntoView({behavior:"smooth"})):console.warn(`No heading found for slug: ${t.slug}`)})}),setInterval(this.updatePositionAndStyle,100),window.addEventListener("scroll",this.updatePositionAndStyle)}}customElements.define("toc-heading",l);</script> <docs-toc class="not-prose docs-toc block mt-3 astro-u7fw3ohz astro-xsy4z74c"> <h2 class="text-foreground font-semibold astro-xsy4z74c">MIND GARDEN</h2> <ul class="mt-4 flex flex-col gap-y-5 astro-xsy4z74c"> <li class="astro-xsy4z74c"> <h3 class="text-muted-foreground text-xs tracking-widest uppercase astro-xsy4z74c">Gen AI</h3> <ul class="mt-2 flex flex-col astro-xsy4z74c"> <li class="docs-item flex relative ms-2 px-3 py-1 text-foreground/75 transition-all rounded-2xl astro-xsy4z74c"> <a class="flex-1 hover:text-foreground astro-xsy4z74c" href="/mind-garden/gen-ai/text-to-speech-fine-tuning"> Fine-tuning TTS Models </a> </li> </ul> </li> </ul> </docs-toc>  <script type="module">class e extends HTMLElement{link="";constructor(){super(),this.link=window.location.pathname}connectedCallback(){this.querySelectorAll("a").forEach(t=>{t.getAttribute("href")===this.link&&t.parentElement?.classList.add("docs-hl")})}}customElements.define("docs-toc",e);</script>  </aside> <div id="sidebar-shade" class="fixed top-0 start-0 w-full h-full astro-scuu7fyy" style="display:none;background-color:#00000091;z-index:31"></div> <article class="min-w-0 grow astro-scuu7fyy">  <div id="content-header" class="animate astro-scuu7fyy">  <h1 class="text-2xl font-medium sm:mb-2 sm:text-3xl astro-u7fw3ohz"> Text-to-Speech (TTS) Fine-tuning </h1> <div class="mt-4 flex flex-wrap gap-x-4 gap-y-2 text-xs leading-6 text-muted-foreground astro-u7fw3ohz"> <blockquote class="text-sm italic text-muted-foreground astro-u7fw3ohz"> <q class="astro-u7fw3ohz">Learn how to fine-tune Text-to-Speech (TTS) models to adapt them to your specific dataset, use case, or desired style and tone.</q> </blockquote> </div>  </div>  <div id="content" class="max-w-none animate mt-8 md:min-w-[45ch] overflow-x-hidden prose text-base text-muted-foreground astro-scuu7fyy">    <h1 id="text-to-speech-tts-fine-tuning">Text-to-Speech (TTS) Fine-tuning<a class="anchor" href="#text-to-speech-tts-fine-tuning">#</a></h1>
<p>Fine-tuning TTS models allows them to adapt to your specific dataset, use case, or desired style and tone. The goal is to customize these models to clone voices, adapt speaking styles and tones, support new languages, handle specific tasks and more. We also support <strong>Speech-to-Text (STT)</strong> models like OpenAI’s Whisper.</p>
<p>With <a href="https://github.com/unslothai/unsloth" rel="nofollow noopener noreferrer" target="_blank">Unsloth<span style="user-select:none"> ↗</span></a>, you can fine-tune TTS models 1.5x faster with 50% less memory than other implementations with Flash Attention 2. This support includes Sesame CSM, Orpheus, and models supported by transformers (e.g. CrisperWhisper, Spark and more).</p>
<p>:::info
Zero-shot cloning captures tone but misses pacing and expression, often sounding robotic and unnatural. Fine-tuning delivers far more accurate and realistic voice replication. <a href="#fine-tuning-voice-models-vs-zero-shot-voice-cloning">Read more here</a>.
:::</p>
<p>We’ve uploaded TTS models (original and quantized variants) to our <a href="https://huggingface.co/collections/unsloth/text-to-speech-tts-models-68007ab12522e96be1e02155" rel="nofollow noopener noreferrer" target="_blank">Hugging Face page<span style="user-select:none"> ↗</span></a>.</p>
<h2 id="fine-tuning-notebooks">Fine-tuning Notebooks:<a class="anchor" href="#fine-tuning-notebooks">#</a></h2>















<div class="overflow-x-auto w-full flex justify-center"><table><thead><tr><th><a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Sesame_CSM_(1B)-TTS.ipynb" rel="nofollow noopener noreferrer" target="_blank">Sesame-CSM (1B)<span style="user-select:none"> ↗</span></a></th><th><a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_(3B)-TTS.ipynb" rel="nofollow noopener noreferrer" target="_blank">Orpheus-TTS (3B)<span style="user-select:none"> ↗</span></a></th><th><a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Whisper.ipynb" rel="nofollow noopener noreferrer" target="_blank">Whisper Large V3<span style="user-select:none"> ↗</span></a> Speech-to-Text (STT)</th></tr></thead><tbody><tr><td><a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Spark_TTS_(0_5B).ipynb" rel="nofollow noopener noreferrer" target="_blank">Spark-TTS (0.5B)<span style="user-select:none"> ↗</span></a></td><td><a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llasa_TTS_(1B).ipynb" rel="nofollow noopener noreferrer" target="_blank">Llasa-TTS (1B)<span style="user-select:none"> ↗</span></a></td><td><a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Oute_TTS_(1B).ipynb" rel="nofollow noopener noreferrer" target="_blank">Oute-TTS (1B)<span style="user-select:none"> ↗</span></a></td></tr></tbody></table></div>
<p>:::success
If you notice that the output duration reaches a maximum of 10 seconds, increase<code>max_new_tokens = 125</code> from its default value of 125. Since 125 tokens corresponds to 10 seconds of audio, you’ll need to set a higher value for longer outputs.
:::</p>
<h2 id="choosing-and-loading-a-tts-model">Choosing and Loading a TTS Model<a class="anchor" href="#choosing-and-loading-a-tts-model">#</a></h2>
<p>For TTS, smaller models are often preferred due to lower latency and faster inference for end users. Fine-tuning a model under 3B parameters is often ideal, and our primary examples uses Sesame-CSM (1B) and Orpheus-TTS (3B), a Llama-based speech model.</p>
<h3 id="sesame-csm-1b-details">Sesame-CSM (1B) Details<a class="anchor" href="#sesame-csm-1b-details">#</a></h3>
<p><strong>CSM-1B</strong> is a base model, while <strong>Orpheus-ft</strong> is fine-tuned on 8 professional voice actors, making voice consistency the key difference. CSM requires audio context for each speaker to perform well, whereas Orpheus-ft has this consistency built in.</p>
<p>Fine-tuning from a base model like CSM generally needs more compute, while starting from a fine-tuned model like Orpheus-ft offers better results out of the box.</p>
<p>To help with CSM, we’ve added new sampling options and an example showing how to use audio context for improved voice consistency.</p>
<h3 id="orpheus-tts-3b-details">Orpheus-TTS (3B) Details<a class="anchor" href="#orpheus-tts-3b-details">#</a></h3>
<p>Orpheus is pre-trained on a large speech corpus and excels at generating realistic speech with built-in support for emotional cues like laughs and sighs. Its architecture makes it one of the easiest TTS models to utilize and train as it can be exported via llama.cpp meaning it has great compatibility across all inference engines. For unsupported models, you’ll only be able to save the LoRA adapter safetensors.</p>
<h3 id="loading-the-models">Loading the models<a class="anchor" href="#loading-the-models">#</a></h3>
<p>Because voice models are usually small in size, you can train the models using LoRA 16-bit or full fine-tuning FFT which may provide higher quality results. To load it in LoRA 16-bit:</p>
<div class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><pre><code><span class="line"><span style="color:#D73A49;--shiki-dark:#F97583">from</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> unsloth </span><span style="color:#D73A49;--shiki-dark:#F97583">import</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> FastModel</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">model_name </span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#032F62;--shiki-dark:#9ECBFF"> "unsloth/orpheus-3b-0.1-pretrained"</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">model, tokenizer </span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> FastModel.from_pretrained(</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">    model_name,</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">    load_in_4bit</span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#005CC5;--shiki-dark:#79B8FF">False</span><span style="color:#6A737D;--shiki-dark:#6A737D">  # use 4-bit precision (QLoRA)</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">)</span></span></code></pre><span class="language ps-1 pe-3 text-sm bg-muted text-muted-foreground">python</span><button class="copy text-muted-foreground p-1 box-content border rounded bg-primary-foreground" data-code="from unsloth import FastModel

model_name = &#x22;unsloth/orpheus-3b-0.1-pretrained&#x22;
model, tokenizer = FastModel.from_pretrained(
    model_name,
    load_in_4bit=False  # use 4-bit precision (QLoRA)
)" onclick="
          navigator.clipboard.writeText(this.dataset.code);
          this.classList.add(&#x27;copied&#x27;);
          setTimeout(() => this.classList.remove(&#x27;copied&#x27;), 2000)
        "><div class="ready"><svg class="size-5"><use href="/icons/code.svg#mingcute-clipboard-line"></use></svg></div><div class="success hidden"><svg class="size-5"><use href="/icons/code.svg#mingcute-file-check-line"></use></svg></div></button></div>
<p>When this runs, Unsloth will download the model weights if you prefer 8-bit, you could use <code>load_in_8bit = True</code>, or for full fine-tuning set <code>full_finetuning = True</code> (ensure you have enough VRAM). You can also replace the model name with other TTS models.</p>
<p>:::info
<strong>Note:</strong> Orpheus’s tokenizer already includes special tokens for audio output (more on this later). You do <em>not</em> need a separate vocoder – Orpheus will output audio tokens directly, which can be decoded to a waveform.
:::</p>
<h2 id="preparing-your-dataset">Preparing Your Dataset<a class="anchor" href="#preparing-your-dataset">#</a></h2>
<p>At minimum, a TTS fine-tuning dataset consists of <strong>audio clips and their corresponding transcripts</strong> (text). Let’s use the <a href="https://huggingface.co/datasets/MrDragonFox/Elise" rel="nofollow noopener noreferrer" target="_blank"><em>Elise</em> dataset<span style="user-select:none"> ↗</span></a> which is ~3 hour single-speaker English speech corpus. There are two variants:</p>
<ul>
<li><a href="https://huggingface.co/datasets/MrDragonFox/Elise" rel="nofollow noopener noreferrer" target="_blank"><code>MrDragonFox/Elise</code><span style="user-select:none"> ↗</span></a> – an augmented version with <strong>emotion tags</strong> (e.g. &#x3C;sigh>, &#x3C;laughs>) embedded in the transcripts. These tags in angle brackets indicate expressions (laughter, sighs, etc.) and are treated as special tokens by Orpheus’s tokenizer</li>
<li><a href="https://huggingface.co/datasets/Jinsaryko/Elise" rel="nofollow noopener noreferrer" target="_blank"><code>Jinsaryko/Elise</code><span style="user-select:none"> ↗</span></a> – base version with transcripts without special tags.</li>
</ul>
<p>The dataset is organized with one audio and transcript per entry. On Hugging Face, these datasets have fields such as <code>audio</code> (the waveform), <code>text</code> (the transcription), and some metadata (speaker name, pitch stats, etc.). We need to feed Unsloth a dataset of audio-text pairs.</p>
<p>:::success
Instead of solely focusing on tone, cadence, and pitch, the priority should be ensuring your dataset is fully annotated and properly normalized.
:::</p>
<p>:::info
With some models like <strong>Sesame-CSM-1B</strong>, you might notice voice variation across generations using speaker ID 0 because it’s a <strong>base model</strong>—it doesn’t have fixed voice identities. Speaker ID tokens mainly help maintain <strong>consistency within a conversation</strong>, not across separate generations.</p>
<p>To get a consistent voice, provide <strong>contextual examples</strong>, like a few reference audio clips or prior utterances. This helps the model mimic the desired voice more reliably. Without this, variation is expected, even with the same speaker ID.
:::</p>
<p><strong>Option 1: Using Hugging Face Datasets library</strong> – We can load the Elise dataset using Hugging Face’s <code>datasets</code> library:</p>
<div class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><pre><code><span class="line"><span style="color:#D73A49;--shiki-dark:#F97583">from</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> datasets </span><span style="color:#D73A49;--shiki-dark:#F97583">import</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> load_dataset, Audio</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;--shiki-dark:#6A737D"># Load the Elise dataset (e.g., the version with emotion tags)</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">dataset </span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> load_dataset(</span><span style="color:#032F62;--shiki-dark:#9ECBFF">"MrDragonFox/Elise"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">, </span><span style="color:#E36209;--shiki-dark:#FFAB70">split</span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#032F62;--shiki-dark:#9ECBFF">"train"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"><span style="color:#005CC5;--shiki-dark:#79B8FF">print</span><span style="color:#24292E;--shiki-dark:#E1E4E8">(</span><span style="color:#005CC5;--shiki-dark:#79B8FF">len</span><span style="color:#24292E;--shiki-dark:#E1E4E8">(dataset), </span><span style="color:#032F62;--shiki-dark:#9ECBFF">"samples"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">)  </span><span style="color:#6A737D;--shiki-dark:#6A737D"># ~1200 samples in Elise</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;--shiki-dark:#6A737D"># Ensure all audio is at 24 kHz sampling rate (Orpheus’s expected rate)</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">dataset </span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> dataset.cast_column(</span><span style="color:#032F62;--shiki-dark:#9ECBFF">"audio"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">, Audio(</span><span style="color:#E36209;--shiki-dark:#FFAB70">sampling_rate</span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#005CC5;--shiki-dark:#79B8FF">24000</span><span style="color:#24292E;--shiki-dark:#E1E4E8">))</span></span></code></pre><span class="language ps-1 pe-3 text-sm bg-muted text-muted-foreground">python</span><button class="copy text-muted-foreground p-1 box-content border rounded bg-primary-foreground" data-code="from datasets import load_dataset, Audio

# Load the Elise dataset (e.g., the version with emotion tags)
dataset = load_dataset(&#x22;MrDragonFox/Elise&#x22;, split=&#x22;train&#x22;)
print(len(dataset), &#x22;samples&#x22;)  # ~1200 samples in Elise

# Ensure all audio is at 24 kHz sampling rate (Orpheus’s expected rate)
dataset = dataset.cast_column(&#x22;audio&#x22;, Audio(sampling_rate=24000))" onclick="
          navigator.clipboard.writeText(this.dataset.code);
          this.classList.add(&#x27;copied&#x27;);
          setTimeout(() => this.classList.remove(&#x27;copied&#x27;), 2000)
        "><div class="ready"><svg class="size-5"><use href="/icons/code.svg#mingcute-clipboard-line"></use></svg></div><div class="success hidden"><svg class="size-5"><use href="/icons/code.svg#mingcute-file-check-line"></use></svg></div></button></div>
<p>This will download the dataset (~328 MB for ~1.2k samples). Each item in <code>dataset</code> is a dictionary with at least:</p>
<ul>
<li><code>"audio"</code>: the audio clip (waveform array and metadata like sampling rate), and</li>
<li><code>"text"</code>: the transcript string</li>
</ul>
<p>Orpheus supports tags like <code>&#x3C;laugh></code>, <code>&#x3C;chuckle></code>, <code>&#x3C;sigh></code>, <code>&#x3C;cough></code>, <code>&#x3C;sniffle></code>, <code>&#x3C;groan></code>, <code>&#x3C;yawn></code>, <code>&#x3C;gasp></code>, etc. For example: <code>"I missed you &#x3C;laugh> so much!"</code>.  These tags are enclosed in angle brackets and will be treated as special tokens by the model (they match <a href="https://github.com/canopyai/Orpheus-TTS" rel="nofollow noopener noreferrer" target="_blank">Orpheus’s expected tags<span style="user-select:none"> ↗</span></a> like <code>&#x3C;laugh></code> and <code>&#x3C;sigh></code>. During training, the model will learn to associate these tags with the corresponding audio patterns. The Elise dataset with tags already has many of these (e.g., 336 occurrences of “laughs”, 156 of “sighs”, etc. as listed in its card). If your dataset lacks such tags but you want to incorporate them, you can manually annotate the transcripts where the audio contains those expressions.</p>
<p><strong>Option 2: Preparing a custom dataset</strong> – If you have your own audio files and transcripts:</p>
<ul>
<li>
<p>Organize audio clips (WAV/FLAC files) in a folder.</p>
</li>
<li>
<p>Create a CSV or TSV file with columns for file path and transcript. For example:</p>
<div class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><pre><code><span class="line"><span>filename,text</span></span>
<span class="line"><span>0001.wav,Hello there!</span></span>
<span class="line"><span>0002.wav,&#x3C;sigh> I am very tired.</span></span></code></pre><span class="language ps-1 pe-3 text-sm bg-muted text-muted-foreground">plaintext</span><button class="copy text-muted-foreground p-1 box-content border rounded bg-primary-foreground" data-code="filename,text
0001.wav,Hello there!
0002.wav,<sigh> I am very tired." onclick="
          navigator.clipboard.writeText(this.dataset.code);
          this.classList.add(&#x27;copied&#x27;);
          setTimeout(() => this.classList.remove(&#x27;copied&#x27;), 2000)
        "><div class="ready"><svg class="size-5"><use href="/icons/code.svg#mingcute-clipboard-line"></use></svg></div><div class="success hidden"><svg class="size-5"><use href="/icons/code.svg#mingcute-file-check-line"></use></svg></div></button></div>
</li>
<li>
<p>Use <code>load_dataset("csv", data_files="mydata.csv", split="train")</code> to load it. You might need to tell the dataset loader how to handle audio paths. An alternative is using the <code>datasets.Audio</code> feature to load audio data on the fly:</p>
<div class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><pre><code><span class="line"><span style="color:#D73A49;--shiki-dark:#F97583">from</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> datasets </span><span style="color:#D73A49;--shiki-dark:#F97583">import</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> Audio</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">dataset </span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> load_dataset(</span><span style="color:#032F62;--shiki-dark:#9ECBFF">"csv"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">, </span><span style="color:#E36209;--shiki-dark:#FFAB70">data_files</span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#032F62;--shiki-dark:#9ECBFF">"mydata.csv"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">, </span><span style="color:#E36209;--shiki-dark:#FFAB70">split</span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#032F62;--shiki-dark:#9ECBFF">"train"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">dataset </span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> dataset.cast_column(</span><span style="color:#032F62;--shiki-dark:#9ECBFF">"filename"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">, Audio(</span><span style="color:#E36209;--shiki-dark:#FFAB70">sampling_rate</span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#005CC5;--shiki-dark:#79B8FF">24000</span><span style="color:#24292E;--shiki-dark:#E1E4E8">))</span></span></code></pre><span class="language ps-1 pe-3 text-sm bg-muted text-muted-foreground">python</span><button class="copy text-muted-foreground p-1 box-content border rounded bg-primary-foreground" data-code="from datasets import Audio
dataset = load_dataset(&#x22;csv&#x22;, data_files=&#x22;mydata.csv&#x22;, split=&#x22;train&#x22;)
dataset = dataset.cast_column(&#x22;filename&#x22;, Audio(sampling_rate=24000))" onclick="
          navigator.clipboard.writeText(this.dataset.code);
          this.classList.add(&#x27;copied&#x27;);
          setTimeout(() => this.classList.remove(&#x27;copied&#x27;), 2000)
        "><div class="ready"><svg class="size-5"><use href="/icons/code.svg#mingcute-clipboard-line"></use></svg></div><div class="success hidden"><svg class="size-5"><use href="/icons/code.svg#mingcute-file-check-line"></use></svg></div></button></div>
<p>Then <code>dataset[i]["audio"]</code> will contain the audio array.</p>
</li>
<li>
<p><strong>Ensure transcripts are normalized</strong> (no unusual characters that the tokenizer might not know, except the emotion tags if used). Also ensure all audio have a consistent sampling rate (resample them if necessary to the target rate the model expects, e.g. 24kHz for Orpheus).</p>
</li>
</ul>
<p>In summary, for <strong>dataset preparation</strong>:</p>
<ul>
<li>You need a <strong>list of (audio, text)</strong> pairs.</li>
<li>Use the HF <code>datasets</code> library to handle loading and optional preprocessing (like resampling).</li>
<li>Include any <strong>special tags</strong> in the text that you want the model to learn (ensure they are in <code>&#x3C;angle_brackets></code> format so the model treats them as distinct tokens).</li>
<li>(Optional) If multi-speaker, you could include a speaker ID token in the text or use a separate speaker embedding approach, but that’s beyond this basic guide (Elise is single-speaker).</li>
</ul>
<h2 id="fine-tuning-tts-with-unsloth">Fine-Tuning TTS with Unsloth<a class="anchor" href="#fine-tuning-tts-with-unsloth">#</a></h2>
<p>Now, let’s start fine-tuning! We’ll illustrate using Python code (which you can run in a Jupyter notebook, Colab, etc.).</p>
<p><strong>Step 1: Load the Model and Dataset</strong></p>
<p>In all our  TTS notebooks, we enable LoRA (16-bit) training and disable QLoRA (4-bit) training with: <code>load_in_4bit = False</code>. This is so the model can usually learn your dataset better and have higher accuracy.</p>
<div class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><pre><code><span class="line"><span style="color:#D73A49;--shiki-dark:#F97583">from</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> unsloth </span><span style="color:#D73A49;--shiki-dark:#F97583">import</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> FastLanguageModel</span></span>
<span class="line"><span style="color:#D73A49;--shiki-dark:#F97583">import</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> torch</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">dtype </span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#005CC5;--shiki-dark:#79B8FF"> None</span><span style="color:#6A737D;--shiki-dark:#6A737D"> # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">load_in_4bit </span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#005CC5;--shiki-dark:#79B8FF"> False</span><span style="color:#6A737D;--shiki-dark:#6A737D"> # Use 4bit quantization to reduce memory usage. Can be False.</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">model, tokenizer </span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> FastLanguageModel.from_pretrained(</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">    model_name</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#032F62;--shiki-dark:#9ECBFF"> "unsloth/orpheus-3b-0.1-ft"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">    max_seq_length</span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#005CC5;--shiki-dark:#79B8FF"> 2048</span><span style="color:#24292E;--shiki-dark:#E1E4E8">, </span><span style="color:#6A737D;--shiki-dark:#6A737D"># Choose any for long context!</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">    dtype</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> dtype,</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">    load_in_4bit</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> load_in_4bit,</span></span>
<span class="line"><span style="color:#6A737D;--shiki-dark:#6A737D">    #token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#D73A49;--shiki-dark:#F97583">from</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> datasets </span><span style="color:#D73A49;--shiki-dark:#F97583">import</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> load_dataset</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">dataset </span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> load_dataset(</span><span style="color:#032F62;--shiki-dark:#9ECBFF">"MrDragonFox/Elise"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">, </span><span style="color:#E36209;--shiki-dark:#FFAB70">split</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#032F62;--shiki-dark:#9ECBFF"> "train"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">)</span></span></code></pre><span class="language ps-1 pe-3 text-sm bg-muted text-muted-foreground">python</span><button class="copy text-muted-foreground p-1 box-content border rounded bg-primary-foreground" data-code="from unsloth import FastLanguageModel
import torch
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = &#x22;unsloth/orpheus-3b-0.1-ft&#x22;,
    max_seq_length= 2048, # Choose any for long context!
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    #token = &#x22;hf_...&#x22;, # use one if using gated models like meta-llama/Llama-2-7b-hf
)

from datasets import load_dataset
dataset = load_dataset(&#x22;MrDragonFox/Elise&#x22;, split = &#x22;train&#x22;)" onclick="
          navigator.clipboard.writeText(this.dataset.code);
          this.classList.add(&#x27;copied&#x27;);
          setTimeout(() => this.classList.remove(&#x27;copied&#x27;), 2000)
        "><div class="ready"><svg class="size-5"><use href="/icons/code.svg#mingcute-clipboard-line"></use></svg></div><div class="success hidden"><svg class="size-5"><use href="/icons/code.svg#mingcute-file-check-line"></use></svg></div></button></div>
<p>:::info
If memory is very limited or if dataset is large, you can stream or load in chunks. Here, 3h of audio easily fits in RAM. If using your own dataset CSV, load it similarly.
:::</p>
<p><strong>Step 2: Advanced - Preprocess the data for training (Optional)</strong></p>
<p>We need to prepare inputs for the Trainer. For text-to-speech, one approach is to train the model in a causal manner: concatenate text and audio token IDs as the target sequence. However, since Orpheus is a decoder-only LLM that outputs audio, we can feed the text as input (context) and have the audio token ids as labels. In practice, Unsloth’s integration might do this automatically if the model’s config identifies it as text-to-speech. If not, we can do something like:</p>
<div class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><pre><code><span class="line"><span style="color:#6A737D;--shiki-dark:#6A737D"># Tokenize the text transcripts</span></span>
<span class="line"><span style="color:#D73A49;--shiki-dark:#F97583">def</span><span style="color:#6F42C1;--shiki-dark:#B392F0"> preprocess_function</span><span style="color:#24292E;--shiki-dark:#E1E4E8">(example):</span></span>
<span class="line"><span style="color:#6A737D;--shiki-dark:#6A737D">    # Tokenize the text (keep the special tokens like &#x3C;laugh> intact)</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">    tokens </span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> tokenizer(example[</span><span style="color:#032F62;--shiki-dark:#9ECBFF">"text"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">], </span><span style="color:#E36209;--shiki-dark:#FFAB70">return_tensors</span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#032F62;--shiki-dark:#9ECBFF">"pt"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"><span style="color:#6A737D;--shiki-dark:#6A737D">    # Flatten to list of token IDs</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">    input_ids </span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> tokens[</span><span style="color:#032F62;--shiki-dark:#9ECBFF">"input_ids"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">].squeeze(</span><span style="color:#005CC5;--shiki-dark:#79B8FF">0</span><span style="color:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"><span style="color:#6A737D;--shiki-dark:#6A737D">    # The model will generate audio tokens after these text tokens.</span></span>
<span class="line"><span style="color:#6A737D;--shiki-dark:#6A737D">    # For training, we can set labels equal to input_ids (so it learns to predict next token).</span></span>
<span class="line"><span style="color:#6A737D;--shiki-dark:#6A737D">    # But that only covers text tokens predicting the next text token (which might be an audio token or end).</span></span>
<span class="line"><span style="color:#6A737D;--shiki-dark:#6A737D">    # A more sophisticated approach: append a special token indicating start of audio, and let the model generate the rest.</span></span>
<span class="line"><span style="color:#6A737D;--shiki-dark:#6A737D">    # For simplicity, use the same input as labels (the model will learn to output the sequence given itself).</span></span>
<span class="line"><span style="color:#D73A49;--shiki-dark:#F97583">    return</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> {</span><span style="color:#032F62;--shiki-dark:#9ECBFF">"input_ids"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">: input_ids, </span><span style="color:#032F62;--shiki-dark:#9ECBFF">"labels"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">: input_ids}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">train_data </span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> dataset.map(preprocess_function, </span><span style="color:#E36209;--shiki-dark:#FFAB70">remove_columns</span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#24292E;--shiki-dark:#E1E4E8">dataset.column_names)</span></span></code></pre><span class="language ps-1 pe-3 text-sm bg-muted text-muted-foreground">python</span><button class="copy text-muted-foreground p-1 box-content border rounded bg-primary-foreground" data-code="# Tokenize the text transcripts
def preprocess_function(example):
    # Tokenize the text (keep the special tokens like <laugh> intact)
    tokens = tokenizer(example[&#x22;text&#x22;], return_tensors=&#x22;pt&#x22;)
    # Flatten to list of token IDs
    input_ids = tokens[&#x22;input_ids&#x22;].squeeze(0)
    # The model will generate audio tokens after these text tokens.
    # For training, we can set labels equal to input_ids (so it learns to predict next token).
    # But that only covers text tokens predicting the next text token (which might be an audio token or end).
    # A more sophisticated approach: append a special token indicating start of audio, and let the model generate the rest.
    # For simplicity, use the same input as labels (the model will learn to output the sequence given itself).
    return {&#x22;input_ids&#x22;: input_ids, &#x22;labels&#x22;: input_ids}

train_data = dataset.map(preprocess_function, remove_columns=dataset.column_names)" onclick="
          navigator.clipboard.writeText(this.dataset.code);
          this.classList.add(&#x27;copied&#x27;);
          setTimeout(() => this.classList.remove(&#x27;copied&#x27;), 2000)
        "><div class="ready"><svg class="size-5"><use href="/icons/code.svg#mingcute-clipboard-line"></use></svg></div><div class="success hidden"><svg class="size-5"><use href="/icons/code.svg#mingcute-file-check-line"></use></svg></div></button></div>
<p>:::info
The above is a simplification. In reality, to fine-tune Orpheus properly, you would need the <em>audio tokens as part of the training labels</em>. Orpheus’s pre-training likely involved converting audio to discrete tokens (via an audio codec) and training the model to predict those given the preceding text. For fine-tuning on new voice data, you would similarly need to obtain the audio tokens for each clip (using Orpheus’s audio codec). The Orpheus GitHub provides a script for data processing – it encodes audio into sequences of <code>&#x3C;custom_token_x></code> tokens.
:::</p>
<p>However, <strong>Unsloth may abstract this away</strong>: if the model is a FastModel with an associated processor that knows how to handle audio, it might automatically encode the audio in the dataset to tokens. If not, you’d have to manually encode each audio clip to token IDs (using Orpheus’s codebook). This is an advanced step beyond this guide, but keep in mind that simply using text tokens won’t teach the model the actual audio – it needs to match the audio patterns.</p>
<p>Let’s assume Unsloth provides a way to feed audio directly (for example, by setting <code>processor</code> and passing the audio array). If Unsloth does not yet support automatic audio tokenization, you might need to use the Orpheus repository’s <code>encode_audio</code> function to get token sequences for the audio, then use those as labels. (The dataset entries do have <code>phonemes</code> and some acoustic features which suggests a pipeline.)</p>
<p><strong>Step 3: Set up training arguments and Trainer</strong></p>
<div class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><pre><code><span class="line"><span style="color:#D73A49;--shiki-dark:#F97583">from</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> transformers </span><span style="color:#D73A49;--shiki-dark:#F97583">import</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> TrainingArguments,Trainer,DataCollatorForSeq2Seq</span></span>
<span class="line"><span style="color:#D73A49;--shiki-dark:#F97583">from</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> unsloth </span><span style="color:#D73A49;--shiki-dark:#F97583">import</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> is_bfloat16_supported</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">trainer </span><span style="color:#D73A49;--shiki-dark:#F97583">=</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> Trainer(</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">    model</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> model,</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">    train_dataset</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> dataset,</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">    args</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> TrainingArguments(</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">        per_device_train_batch_size</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#005CC5;--shiki-dark:#79B8FF"> 1</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">        gradient_accumulation_steps</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#005CC5;--shiki-dark:#79B8FF"> 4</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">        warmup_steps</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#005CC5;--shiki-dark:#79B8FF"> 5</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#6A737D;--shiki-dark:#6A737D">        # num_train_epochs = 1, # Set this for 1 full training run.</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">        max_steps</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#005CC5;--shiki-dark:#79B8FF"> 60</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">        learning_rate</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#005CC5;--shiki-dark:#79B8FF"> 2e-4</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">        fp16</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#D73A49;--shiki-dark:#F97583"> not</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> is_bfloat16_supported(),</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">        bf16</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#24292E;--shiki-dark:#E1E4E8"> is_bfloat16_supported(),</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">        logging_steps</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#005CC5;--shiki-dark:#79B8FF"> 1</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">        optim</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#032F62;--shiki-dark:#9ECBFF"> "adamw_8bit"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">        weight_decay</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#005CC5;--shiki-dark:#79B8FF"> 0.01</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">        lr_scheduler_type</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#032F62;--shiki-dark:#9ECBFF"> "linear"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">        seed</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#005CC5;--shiki-dark:#79B8FF"> 3407</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">        output_dir</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#032F62;--shiki-dark:#9ECBFF"> "outputs"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="color:#E36209;--shiki-dark:#FFAB70">        report_to</span><span style="color:#D73A49;--shiki-dark:#F97583"> =</span><span style="color:#032F62;--shiki-dark:#9ECBFF"> "none"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">, </span><span style="color:#6A737D;--shiki-dark:#6A737D"># Use this for WandB etc</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">    ),</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">)</span></span></code></pre><span class="language ps-1 pe-3 text-sm bg-muted text-muted-foreground">python</span><button class="copy text-muted-foreground p-1 box-content border rounded bg-primary-foreground" data-code="from transformers import TrainingArguments,Trainer,DataCollatorForSeq2Seq
from unsloth import is_bfloat16_supported

trainer = Trainer(
    model = model,
    train_dataset = dataset,
    args = TrainingArguments(
        per_device_train_batch_size = 1,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        # num_train_epochs = 1, # Set this for 1 full training run.
        max_steps = 60,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = &#x22;adamw_8bit&#x22;,
        weight_decay = 0.01,
        lr_scheduler_type = &#x22;linear&#x22;,
        seed = 3407,
        output_dir = &#x22;outputs&#x22;,
        report_to = &#x22;none&#x22;, # Use this for WandB etc
    ),
)" onclick="
          navigator.clipboard.writeText(this.dataset.code);
          this.classList.add(&#x27;copied&#x27;);
          setTimeout(() => this.classList.remove(&#x27;copied&#x27;), 2000)
        "><div class="ready"><svg class="size-5"><use href="/icons/code.svg#mingcute-clipboard-line"></use></svg></div><div class="success hidden"><svg class="size-5"><use href="/icons/code.svg#mingcute-file-check-line"></use></svg></div></button></div>
<p>We do 60 steps to speed things up, but you can set <code>num_train_epochs=1</code> for a full run, and turn off <code>max_steps=None</code>. Using a per_device_train_batch_size >1 may lead to errors if multi-GPU setup to avoid issues, ensure CUDA_VISIBLE_DEVICES is set to a single GPU (e.g., CUDA_VISIBLE_DEVICES=0). Adjust as needed.</p>
<p><strong>Step 4: Begin fine-tuning</strong></p>
<p>This will start the training loop. You should see logs of loss every 50 steps (as set by <code>logging_steps</code>). The training might take some time depending on GPU – for example, on a Colab T4 GPU, a few epochs on 3h of data may take 1-2 hours. Unsloth’s optimizations will make it faster than standard HF training.</p>
<p><strong>Step 5: Save the fine-tuned model</strong></p>
<p>After training completes (or if you stop it mid-way when you feel it’s sufficient), save the model. This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!</p>
<div class="astro-code astro-code-themes github-light github-dark" style="background-color:#fff;--shiki-dark-bg:#24292e;color:#24292e;--shiki-dark:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><pre><code><span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">model.save_pretrained(</span><span style="color:#032F62;--shiki-dark:#9ECBFF">"lora_model"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">)  </span><span style="color:#6A737D;--shiki-dark:#6A737D"># Local saving</span></span>
<span class="line"><span style="color:#24292E;--shiki-dark:#E1E4E8">tokenizer.save_pretrained(</span><span style="color:#032F62;--shiki-dark:#9ECBFF">"lora_model"</span><span style="color:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"><span style="color:#6A737D;--shiki-dark:#6A737D"># model.push_to_hub("your_name/lora_model", token = "...") # Online saving</span></span>
<span class="line"><span style="color:#6A737D;--shiki-dark:#6A737D"># tokenizer.push_to_hub("your_name/lora_model", token = "...") # Online saving</span></span></code></pre><span class="language ps-1 pe-3 text-sm bg-muted text-muted-foreground">python</span><button class="copy text-muted-foreground p-1 box-content border rounded bg-primary-foreground" data-code="model.save_pretrained(&#x22;lora_model&#x22;)  # Local saving
tokenizer.save_pretrained(&#x22;lora_model&#x22;)
# model.push_to_hub(&#x22;your_name/lora_model&#x22;, token = &#x22;...&#x22;) # Online saving
# tokenizer.push_to_hub(&#x22;your_name/lora_model&#x22;, token = &#x22;...&#x22;) # Online saving" onclick="
          navigator.clipboard.writeText(this.dataset.code);
          this.classList.add(&#x27;copied&#x27;);
          setTimeout(() => this.classList.remove(&#x27;copied&#x27;), 2000)
        "><div class="ready"><svg class="size-5"><use href="/icons/code.svg#mingcute-clipboard-line"></use></svg></div><div class="success hidden"><svg class="size-5"><use href="/icons/code.svg#mingcute-file-check-line"></use></svg></div></button></div>
<p>This saves the model weights (for LoRA, it might save only adapter weights if the base is not fully fine-tuned). If you used <code>--push_model</code> in CLI or <code>trainer.push_to_hub()</code>, you could upload it to Hugging Face Hub directly.</p>
<p>Now you should have a fine-tuned TTS model in the directory. The next step is to test it out and if supported, you can use llama.cpp to convert it into a GGUF file.</p>
<h2 id="fine-tuning-voice-models-vs-zero-shot-voice-cloning">Fine-tuning Voice models vs. Zero-shot voice cloning<a class="anchor" href="#fine-tuning-voice-models-vs-zero-shot-voice-cloning">#</a></h2>
<p>People say you can clone a voice with just 30 seconds of audio using models like XTTS - no training required. That’s technically true, but it misses the point.</p>
<p>Zero-shot voice cloning, which is also available in models like Orpheus and CSM, is an approximation. It captures the general <strong>tone and timbre</strong> of a speaker’s voice, but it doesn’t reproduce the full expressive range. You lose details like speaking speed, phrasing, vocal quirks, and the subtleties of prosody - things that give a voice its <strong>personality and uniqueness</strong>.</p>
<p>If you just want a different voice and are fine with the same delivery patterns, zero-shot is usually good enough. But the speech will still follow the <strong>model’s style</strong>, not the speaker’s.</p>
<p>For anything more personalized or expressive, you need training with methods like LoRA to truly capture how someone speaks.</p>   </div> </article> </main> <div class="bottom mt-6 items-start gap-x-10 md:flex astro-scuu7fyy">  <div class="flex-1 text-muted-foreground md:min-w-[50ch] astro-scuu7fyy"> <div class="astro-u7fw3ohz"> <a class="group inline-flex items-center gap-x-1 rounded-lg bg-muted border px-2 py-1 text-sm text-muted-foreground transition-all hover:bg-primary-foreground no-underline px-3 py-2 astro-u7fw3ohz" href="https://github.com/yohn-maistre/yohn-maistre.github.io/edit/main/src/content/mind-garden/training-and-fine-tuning/text-to-speech-fine-tuning.mdx" data-astro-prefetch="true" style="font-size:1rem"> <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" class="astro-u7fw3ohz"><!-- Icon from MingCute Icon by MingCute Design - https://github.com/Richard9394/MingCute/blob/main/LICENSE --><g fill="none" fill-rule="evenodd" class="astro-u7fw3ohz"><path d="m12.594 23.258l-.012.002l-.071.035l-.02.004l-.014-.004l-.071-.036q-.016-.004-.024.006l-.004.01l-.017.428l.005.02l.01.013l.104.074l.015.004l.012-.004l.104-.074l.012-.016l.004-.017l-.017-.427q-.004-.016-.016-.018m.264-.113l-.014.002l-.184.093l-.01.01l-.003.011l.018.43l.005.012l.008.008l.201.092q.019.005.029-.008l.004-.014l-.034-.614q-.005-.019-.02-.022m-.715.002a.02.02 0 0 0-.027.006l-.006.014l-.034.614q.001.018.017.024l.015-.002l.201-.093l.01-.008l.003-.011l.018-.43l-.003-.012l-.01-.01z" class="astro-u7fw3ohz"></path><path fill="currentColor" d="M13.896 3.03a2 2 0 0 1 2.701-.117l.127.117l4.243 4.243a2 2 0 0 1 .117 2.7l-.117.128l-10.314 10.314a2 2 0 0 1-1.238.578L9.239 21H4.006a1.01 1.01 0 0 1-1.004-.9l-.006-.11v-5.233a2 2 0 0 1 .467-1.284l.12-.13L13.895 3.03ZM12.17 7.584l-7.174 7.174V19H9.24l7.174-7.174l-4.243-4.243Zm3.14-3.14L13.584 6.17l4.243 4.243l1.726-1.726z" class="astro-u7fw3ohz"></path></g></svg>  
Edit on GitHub
    </a>  <div class="rounded-xl border px-4 py-3 mt-3 sm:py-4 md:mt-6 astro-u7fw3ohz"> <docs-toc class="not-prose astro-u7fw3ohz astro-xsy4z74c"> <h2 class="text-foreground font-semibold astro-xsy4z74c">MIND GARDEN</h2> <ul class="mt-4 flex flex-col gap-y-5 astro-xsy4z74c"> <li class="astro-xsy4z74c"> <h3 class="text-muted-foreground text-xs tracking-widest uppercase astro-xsy4z74c">Gen AI</h3> <ul class="mt-2 flex flex-col astro-xsy4z74c"> <li class="docs-item flex relative ms-2 px-3 py-1 text-foreground/75 transition-all rounded-2xl astro-xsy4z74c"> <a class="flex-1 hover:text-foreground astro-xsy4z74c" href="/mind-garden/gen-ai/text-to-speech-fine-tuning"> Fine-tuning TTS Models </a> </li> </ul> </li> </ul> </docs-toc>   </div> </div> </div> <div class="min-w-48 basis-60 astro-scuu7fyy">  </div> </div> <div class="z-10 group fixed bottom-8 end-4 transition-all flex flex-col gap-y-4 sm:end-8 astro-jxfdehre" id="action-buttons"> <button aria-label="Toggle sidebar" class="size-10 flex items-center justify-center rounded-full border-2 border-transparent bg-muted text-muted-foreground duration-300 hover:border-border/75 md:hidden sm:size-12 astro-scuu7fyy" id="sidebar-btn"> <svg aria-hidden="true" class="astro-scuu7fyy astro-hn7k2gay" width="22" height="22" viewBox="0 0 24 24" fill="currentColor" style="--sl-icon-size: 1em;"><g fill="none"><path d="m12.593 23.258l-.011.002l-.071.035l-.02.004l-.014-.004l-.071-.035q-.016-.005-.024.005l-.004.01l-.017.428l.005.02l.01.013l.104.074l.015.004l.012-.004l.104-.074l.012-.016l.004-.017l-.017-.427q-.004-.016-.017-.018m.265-.113l-.013.002l-.185.093l-.01.01l-.003.011l.018.43l.005.012l.008.007l.201.093q.019.005.029-.008l.004-.014l-.034-.614q-.005-.018-.02-.022m-.715.002a.02.02 0 0 0-.027.006l-.006.014l-.034.614q.001.018.017.024l.015-.002l.201-.093l.01-.008l.004-.011l.017-.43l-.003-.012l-.01-.01z"/><path fill="currentColor" d="M4.5 17.5a1.5 1.5 0 1 1 0 3a1.5 1.5 0 0 1 0-3M20 18a1 1 0 1 1 0 2H9a1 1 0 1 1 0-2zM4.5 10.5a1.5 1.5 0 1 1 0 3a1.5 1.5 0 0 1 0-3M20 11a1 1 0 0 1 .117 1.993L20 13H9a1 1 0 0 1-.117-1.993L9 11zM4.5 3.5a1.5 1.5 0 1 1 0 3a1.5 1.5 0 0 1 0-3M20 4a1 1 0 0 1 .117 1.993L20 6H9a1 1 0 0 1-.117-1.993L9 4z"/></g></svg>  </button> <button aria-label="Back to Top" class="relative size-10 flex items-center justify-center rounded-full border-2 border-transparent bg-muted text-muted-foreground duration-300 hover:border-border/75 sm:size-12 opacity-0 astro-jxfdehre" id="to-top-btn"> <div class="top-text absolute bottom-0 end-0 start-0 top-0 flex items-center justify-center group-[.ended]:opacity-0 astro-jxfdehre"> <span class="text astro-jxfdehre">10</span> <span class="text-xs astro-jxfdehre">%</span> </div> <svg aria-hidden="true" class="top-icon group-[.ended]:opacity-100 astro-jxfdehre astro-hn7k2gay" width="22" height="22" viewBox="0 0 24 24" fill="currentColor" style="--sl-icon-size: 1em;"><g fill="none" fill-rule="evenodd"><path d="M24 0v24H0V0zM12.593 23.258l-.011.002l-.071.035l-.02.004l-.014-.004l-.071-.035q-.016-.005-.024.005l-.004.01l-.017.428l.005.02l.01.013l.104.074l.015.004l.012-.004l.104-.074l.012-.016l.004-.017l-.017-.427q-.004-.016-.017-.018m.265-.113l-.013.002l-.185.093l-.01.01l-.003.011l.018.43l.005.012l.008.007l.201.093q.019.005.029-.008l.004-.014l-.034-.614q-.005-.019-.02-.022m-.715.002a.02.02 0 0 0-.027.006l-.006.014l-.034.614q.001.018.017.024l.015-.002l.201-.093l.01-.008l.004-.011l.017-.43l-.003-.012l-.01-.01z"/><path fill="currentColor" d="M11.293 8.293a1 1 0 0 1 1.414 0l5.657 5.657a1 1 0 0 1-1.414 1.414L12 10.414l-4.95 4.95a1 1 0 0 1-1.414-1.414z"/></g></svg>  </button> </div> <script>(function(){const headerName = "content-header";
const contentName = "content";
const needPercent = true;

  const actionBtns = document.getElementById('action-buttons')
  const scrollBtn = document.getElementById('to-top-btn')
  const scrollPercentEl = scrollBtn.children[0].children[0]
  const targetHeader = document.getElementById(headerName)
  const articleElement = document.getElementById(contentName)

  // scroll to top
  function callback(entries) {
    entries.forEach((entry) => {
      // only show the action buttons when the heading is out of view
      actionBtns.classList.toggle('show', !entry.isIntersecting)
    })
  }
  scrollBtn.addEventListener('click', () => {
    document.documentElement.scrollTo({ behavior: 'smooth', top: 0 })
  })
  if (targetHeader) {
    const observer = new IntersectionObserver(callback)
    observer.observe(targetHeader)
  } else {
    console.error(`Element with ID ${headerName} not found.`)
  }
  if (needPercent) {
    // scroll percentage
    const scrollHeight = articleElement.scrollHeight // total height
    const articleTop = articleElement.offsetTop // article top
    const clientHeight = document.documentElement.clientHeight // client height

    function calculateScrollPercent() {
      const scrollTop = document.documentElement.scrollTop || document.body.scrollTop
      if (scrollTop < articleTop) return
      return Math.round(((scrollTop - articleTop) / (scrollHeight - clientHeight)) * 100)
    }

    document.addEventListener('scroll', () => {
      const scrollPercent = calculateScrollPercent()
      if (scrollPercent === undefined) return
      scrollPercentEl.innerText = scrollPercent.toString()

      // If percent is 100, percent won't need to show
      actionBtns.classList.toggle('ended', scrollPercent > 100)
    })
  } else {
    actionBtns.classList.add('ended')
  }
})();</script>   </div> <footer class="mx-auto mb-5 mt-16 w-full"> <div class="border-t pt-5"> <div class="flex items-center gap-y-3 max-sm:flex-col sm:justify-between sm:gap-y-0"> <div class="flex items-center gap-x-4 gap-y-2 text-muted-foreground max-sm:flex-col [&_a]:text-foreground">    <div> © 2025 Yose Marthin Giyay 
&
<span> <span> <a href="/terms/list" target="_blank"> Site Policy </a> </span> </span>  </div>  </div>  <div class="flex items-center gap-x-4"> <a class="inline-block text-muted-foreground transition-all hover:text-muted-foreground/75" href="https://github.com/yohn-maistre" aria-label="GitHub"> <svg aria-hidden="true" class=" astro-hn7k2gay" width="22" height="22" viewBox="0 0 24 24" fill="currentColor" style="--sl-icon-size: 1em;"><g fill="none"><path d="m12.593 23.258l-.011.002l-.071.035l-.02.004l-.014-.004l-.071-.035q-.016-.005-.024.005l-.004.01l-.017.428l.005.02l.01.013l.104.074l.015.004l.012-.004l.104-.074l.012-.016l.004-.017l-.017-.427q-.004-.016-.017-.018m.265-.113l-.013.002l-.185.093l-.01.01l-.003.011l.018.43l.005.012l.008.007l.201.093q.019.005.029-.008l.004-.014l-.034-.614q-.005-.018-.02-.022m-.715.002a.02.02 0 0 0-.027.006l-.006.014l-.034.614q.001.018.017.024l.015-.002l.201-.093l.01-.008l.004-.011l.017-.43l-.003-.012l-.01-.01z"/><path fill="currentColor" d="M6.315 6.176c-.25-.638-.24-1.367-.129-2.034a6.8 6.8 0 0 1 2.12 1.07c.28.214.647.283.989.18A9.3 9.3 0 0 1 12 5c.961 0 1.874.14 2.703.391c.342.104.709.034.988-.18a6.8 6.8 0 0 1 2.119-1.07c.111.667.12 1.396-.128 2.033c-.15.384-.075.826.208 1.14C18.614 8.117 19 9.04 19 10c0 2.114-1.97 4.187-5.134 4.818c-.792.158-1.101 1.155-.495 1.726c.389.366.629.882.629 1.456v3a1 1 0 0 0 2 0v-3c0-.57-.12-1.112-.334-1.603C18.683 15.35 21 12.993 21 10c0-1.347-.484-2.585-1.287-3.622c.21-.82.191-1.646.111-2.28c-.071-.568-.17-1.312-.57-1.756c-.595-.659-1.58-.271-2.28-.032a9 9 0 0 0-2.125 1.045A11.4 11.4 0 0 0 12 3c-.994 0-1.953.125-2.851.356a9 9 0 0 0-2.125-1.045c-.7-.24-1.686-.628-2.281.031c-.408.452-.493 1.137-.566 1.719l-.005.038c-.08.635-.098 1.462.112 2.283C3.484 7.418 3 8.654 3 10c0 2.992 2.317 5.35 5.334 6.397A4 4 0 0 0 8 17.98l-.168.034c-.717.099-1.176.01-1.488-.122c-.76-.322-1.152-1.133-1.63-1.753c-.298-.385-.732-.866-1.398-1.088a1 1 0 0 0-.632 1.898c.558.186.944 1.142 1.298 1.566c.373.448.869.916 1.58 1.218c.682.29 1.483.393 2.438.276V21a1 1 0 0 0 2 0v-3c0-.574.24-1.09.629-1.456c.607-.572.297-1.568-.495-1.726C6.969 14.187 5 12.114 5 10c0-.958.385-1.881 1.108-2.684c.283-.314.357-.756.207-1.14"/></g></svg>  </a><a class="inline-block text-muted-foreground transition-all hover:text-muted-foreground/75" href="/rss.xml" aria-label="RSS"> <svg aria-hidden="true" class=" astro-hn7k2gay" width="22" height="22" viewBox="0 0 24 24" fill="currentColor" style="--sl-icon-size: 1em;"><g fill="none"><path d="m12.594 23.258l-.012.002l-.071.035l-.02.004l-.014-.004l-.071-.036q-.016-.004-.024.006l-.004.01l-.017.428l.005.02l.01.013l.104.074l.015.004l.012-.004l.104-.074l.012-.016l.004-.017l-.017-.427q-.004-.016-.016-.018m.264-.113l-.014.002l-.184.093l-.01.01l-.003.011l.018.43l.005.012l.008.008l.201.092q.019.005.029-.008l.004-.014l-.034-.614q-.005-.019-.02-.022m-.715.002a.02.02 0 0 0-.027.006l-.006.014l-.034.614q.001.018.017.024l.015-.002l.201-.093l.01-.008l.003-.011l.018-.43l-.003-.012l-.01-.01z"/><path fill="currentColor" d="M5.5 17a1.5 1.5 0 1 1 0 3a1.5 1.5 0 0 1 0-3m0-14C14.06 3 21 9.94 21 18.5q0 .268-.009.534a1 1 0 0 1-1.999-.068Q19 18.734 19 18.5C19 11.044 12.956 5 5.5 5q-.234 0-.466.008a1 1 0 0 1-.068-1.999Q5.231 3 5.5 3m0 7a8.5 8.5 0 0 1 8.482 9.066a1 1 0 0 1-1.996-.132a6.5 6.5 0 0 0-6.92-6.92a1 1 0 1 1-.132-1.995q.28-.02.566-.019"/></g></svg>  </a> </div> </div> </div> </footer> </div>   </body> </html> <script type="module">const d=document.getElementById("sidebar-btn"),e=document.getElementById("sidebar"),s=document.getElementById("sidebar-shade");s&&s.addEventListener("click",()=>{t()});function t(){e&&e.classList.contains("show")?(e.classList.remove("show"),s.style.display="none"):e&&(e.classList.add("show"),s.style.display="block")}d?.addEventListener("click",()=>{t()});</script>  <script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/pure/medium-zoom.min.umd.js"></script> <script>(function(){const selector = ".prose .zoomable";
const background = "hsl(var(--background) / 0.8)";

  mediumZoom(selector, { background })
})();</script>  